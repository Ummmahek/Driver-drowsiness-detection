{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmx8ogAD8nqpgFm2DEid+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ummmahek/Driver-drowsiness-detection/blob/main/YOLOFACEMARK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrWvYDM1PKDY",
        "outputId": "df077607-7502-4763-d06c-5db3f44dbee9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX1mcw7yxBjY",
        "outputId": "821b853b-f34c-43de-8450-ea65d8189874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Extracting dataset...\n",
            "Dataset extracted to: nthu_ddd_dataset\n"
          ]
        }
      ],
      "source": [
        "# YOLOFaceMark on NTHU-DDD Dataset (with YOLOFaceMark model and EAR/MAR classification)\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# --- 1. DOWNLOAD AND EXTRACT DATASET ---\n",
        "data_url = \"https://universe.roboflow.com/ds/p8jjDsbFGl?key=Bwh2h28aje\"\n",
        "dataset_path = \"nthu_ddd_dataset.zip\"\n",
        "extract_path = \"nthu_ddd_dataset\"\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    response = requests.get(data_url, stream=True)\n",
        "    with open(dataset_path, \"wb\") as file:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            file.write(chunk)\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Dataset extracted to:\", extract_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2D_ikxjXZAX",
        "outputId": "f2936875-e451-48dd-9b48-60025f3b9c01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dlib in /usr/local/lib/python3.11/dist-packages (19.24.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d4ed050",
        "outputId": "238b459f-0fff-4f79-8980-53388efb2214"
      },
      "source": [
        "# YOLOFaceMark on NTHU-DDD Dataset (Complete Pipeline: YOLOFaceMark + Dlib Landmark Bootstrapping + EAR/MAR Drowsiness Classification)\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import zipfile\n",
        "import dlib\n",
        "\n",
        "# --- 1. DOWNLOAD AND EXTRACT DATASET ---\n",
        "data_url = \"https://universe.roboflow.com/ds/p8jjDsbFGl?key=Bwh2h28aje\"\n",
        "dataset_path = \"nthu_ddd_dataset.zip\"\n",
        "extract_path = \"nthu_ddd_dataset\"\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    response = requests.get(data_url, stream=True)\n",
        "    with open(dataset_path, \"wb\") as file:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            file.write(chunk)\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Dataset extracted to:\", extract_path)\n",
        "\n",
        "# --- 2. Dataset with TXT Annotations and Optional Landmark Bootstrapping ---\n",
        "class NTHUDataset(Dataset):\n",
        "    def __init__(self, img_dir, txt_dir, transform=None, use_dlib=False):\n",
        "        self.img_paths = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
        "        self.txt_dir = txt_dir\n",
        "        self.transform = transform if transform else ToTensor()\n",
        "        self.use_dlib = use_dlib\n",
        "        if use_dlib:\n",
        "            self.detector = dlib.get_frontal_face_detector()\n",
        "            predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
        "            if not os.path.exists(predictor_path):\n",
        "                print(\"Downloading dlib model...\")\n",
        "                url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "                r = requests.get(url)\n",
        "                with open(\"temp.bz2\", \"wb\") as f:\n",
        "                    f.write(r.content)\n",
        "                import bz2\n",
        "                with bz2.BZ2File(\"temp.bz2\") as f_in, open(predictor_path, \"wb\") as f_out:\n",
        "                    f_out.write(f_in.read())\n",
        "                os.remove(\"temp.bz2\")\n",
        "            self.predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img_name = os.path.basename(img_path).replace('.jpg', '.txt')\n",
        "        txt_path = os.path.join(self.txt_dir, img_name)\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        # Parse annotation txt\n",
        "        with open(txt_path, 'r') as f:\n",
        "            parts = list(map(float, f.read().strip().split()))\n",
        "        bbox = parts[1:5]  # [cx, cy, w, h] normalized\n",
        "        bbox_abs = [bbox[0]*w, bbox[1]*h, bbox[2]*w, bbox[3]*h]  # absolute\n",
        "\n",
        "        # Bootstrap 68 landmarks using dlib\n",
        "        if self.use_dlib:\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            faces = self.detector(gray)\n",
        "            if faces:\n",
        "                shape = self.predictor(gray, faces[0])\n",
        "                landmarks = np.array([[p.x / w, p.y / h, 1.0] for p in shape.parts()])  # normalize\n",
        "            else:\n",
        "                landmarks = np.zeros((68, 3), dtype=np.float32)\n",
        "        else:\n",
        "            landmarks = np.zeros((68, 3), dtype=np.float32)\n",
        "\n",
        "        img_tensor = self.transform(img_rgb).float() # Convert to Float\n",
        "        target = {\n",
        "            'bbox': torch.tensor(bbox, dtype=torch.float32),\n",
        "            'landmarks': torch.tensor(landmarks.flatten(), dtype=torch.float32)\n",
        "        }\n",
        "        return img_tensor, target, img_path\n",
        "\n",
        "# --- 3. YOLOFaceMark Model Definition ---\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, k, s, p, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class YOLOFaceMark(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            ConvBlock(3, 32),\n",
        "            ConvBlock(32, 64),\n",
        "            nn.MaxPool2d(2),\n",
        "            ConvBlock(64, 128),\n",
        "            nn.MaxPool2d(2),\n",
        "            ConvBlock(128, 256),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.neck = nn.Sequential(\n",
        "            ConvBlock(256, 256),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            ConvBlock(256, 128)\n",
        "        )\n",
        "        self.head_bbox = nn.Conv2d(128, 4, 1) # Changed output channels from 6 to 4\n",
        "        self.head_lmks = nn.Conv2d(128, 204, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.neck(x)\n",
        "        bbox = self.head_bbox(x).mean([2, 3])\n",
        "        lmk = self.head_lmks(x).mean([2, 3])\n",
        "        return bbox, lmk\n",
        "\n",
        "\n",
        "# --- 4. OKS Loss Function for Landmarks ---\n",
        "class OKSLoss(nn.Module):\n",
        "    def __init__(self, s=1.0, k=0.1):\n",
        "        super().__init__()\n",
        "        self.s = s\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, pred, gt):\n",
        "        pred = pred.view(-1, 68, 3)\n",
        "        gt = gt.view(-1, 68, 3)\n",
        "        d = torch.norm(pred[..., :2] - gt[..., :2], dim=2)\n",
        "        oks = torch.exp(-d ** 2 / (2 * self.s ** 2 * self.k ** 2))\n",
        "        mask = (gt[..., 2] > 0).float()\n",
        "        oks = (oks * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
        "        return 1 - oks.mean()\n",
        "\n",
        "\n",
        "# --- 5. EAR / MAR + Classification ---\n",
        "def compute_ear(eye):\n",
        "    d0 = np.linalg.norm(eye[0] - eye[3])\n",
        "    d1 = np.linalg.norm(eye[1] - eye[5])\n",
        "    d2 = np.linalg.norm(eye[2] - eye[4])\n",
        "    return (d1 + d2) / (2.0 * d0)\n",
        "\n",
        "def compute_mar(mouth):\n",
        "    d5 = np.linalg.norm(mouth[2] - mouth[6])\n",
        "    d6 = np.linalg.norm(mouth[0] - mouth[4])\n",
        "    return d5 / d6\n",
        "\n",
        "def classify_drowsiness(landmarks):\n",
        "    eye_pts = [36, 37, 38, 39, 40, 41]\n",
        "    mouth_pts = [60, 61, 62, 63, 64, 65, 66, 67]\n",
        "    eye = np.array([landmarks[i][:2] for i in eye_pts])\n",
        "    mouth = np.array([landmarks[i][:2] for i in mouth_pts])\n",
        "    ear = compute_ear(eye)\n",
        "    mar = compute_mar(mouth)\n",
        "    return (ear < 0.2 or mar > 0.5), ear, mar\n",
        "\n",
        "\n",
        "# --- 6. Training + Evaluation Loop ---\n",
        "def train(model, train_loader, val_loader, epochs=10, lr=1e-3):\n",
        "    # Check if CUDA is available and use GPU, otherwise use CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion_bbox = nn.MSELoss()\n",
        "    criterion_lmk = OKSLoss()\n",
        "\n",
        "    train_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for imgs, targets, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            # Move data to the selected device\n",
        "            imgs = imgs.to(device)\n",
        "            bbox_gt = targets['bbox'].to(device)\n",
        "            lmk_gt = targets['landmarks'].to(device)\n",
        "\n",
        "            bbox_pred, lmk_pred = model(imgs)\n",
        "            loss = criterion_bbox(bbox_pred, bbox_gt) + criterion_lmk(lmk_pred, lmk_gt)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n",
        "        train_losses.append(avg_loss)\n",
        "        # Return train_losses after each epoch to plot progressively\n",
        "\n",
        "\n",
        "    return train_losses\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    # Check if CUDA is available and use GPU, otherwise use CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    TP, FP, TN, FN = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets, _ in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            # Move data to the selected device\n",
        "            imgs = imgs.to(device)\n",
        "            _, lmk_out = model(imgs)\n",
        "            landmarks = lmk_out.view(-1, 68, 3)[0, :, :2].cpu().numpy()\n",
        "            drowsy, ear, mar = classify_drowsiness(landmarks)\n",
        "            pred = int(drowsy)\n",
        "            # Read true label from .txt file (first value is class: 1=drowsy, 0=awake)\n",
        "            label_path = _[0].replace('.jpg', '.txt')\n",
        "            with open(label_path, 'r') as f:\n",
        "                label = int(f.read().strip().split()[0])\n",
        "            if pred == 1 and label == 1: TP += 1\n",
        "            elif pred == 1 and label == 0: FP += 1\n",
        "            elif pred == 0 and label == 0: TN += 1\n",
        "            elif pred == 0 and label == 1: FN += 1\n",
        "    accuracy = (TP + TN) / max(TP + TN + FP + FN, 1)\n",
        "    precision = TP / max(TP + FP, 1)\n",
        "    recall = TP / max(TP + FN, 1)\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "# --- 7. Run the Complete Pipeline ---\n",
        "if __name__ == '__main__':\n",
        "    # Paths\n",
        "    train_img_dir = os.path.join(extract_path, 'train')\n",
        "    valid_img_dir = os.path.join(extract_path, 'valid')\n",
        "\n",
        "    # Data\n",
        "    train_set = NTHUDataset(train_img_dir, train_img_dir, use_dlib=True)\n",
        "    valid_set = NTHUDataset(valid_img_dir, valid_img_dir, use_dlib=True)\n",
        "    train_loader = DataLoader(train_set, batch_size=16, shuffle=True) # Decreased batch size\n",
        "    valid_loader = DataLoader(valid_set, batch_size=16, shuffle=False) # Decreased batch size\n",
        "\n",
        "    # Model\n",
        "    model = YOLOFaceMark()\n",
        "\n",
        "    # Train\n",
        "    train_losses = train(model, train_loader, valid_loader, epochs=10)\n",
        "\n",
        "    # Evaluate\n",
        "    evaluate(model, valid_loader)\n",
        "\n",
        "    # Plot Training Loss\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.title(\"Training Loss vs. Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Extracting dataset...\n",
            "Dataset extracted to: nthu_ddd_dataset\n",
            "Downloading dlib model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|          | 0/98 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}